
# [Tutorial] Data voorbereiden



```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, cache = T)
library(tufte)
library(pander)
source("prepare_session.R")
knitr::opts_chunk$set(out.width = "100%",
                      fig.width = 6, fig.asp = 0.6, tidy='styler',
                      cache = T,
                      echo = T,
                      warning = F)
```



```{r echo = F, include = F}
library(dplyr)
library(forcats)
set.seed(1)
forcats::gss_cat %>% 
	mutate(rincome = ordered(rincome),
		   rincome = fct_relevel(rincome, "Lt $1000", after = 3)) %>%
	mutate(tvhours = tvhours + tvhours*6*sample(size = nrow(.), c(T,F), prob = c(0.001,0.999), replace = T)) %>%
	mutate_all(as.character) %>%
	saveRDS("survey.RDS")
```
## Voor je begint

Tijdens deze tutorial zullen we verschillende r-pakketten gebru., indien nodig.

```{r}
library(ggplot2)
library(dplyr)
library(forcats)
library(mice)
```

Het `forcats` pakket bevat een serie functies om op eenvoudige wijze factoren te manipuleren (waarvan _forcats_ een anagram is). Het `mice` pakket kan worden gebruikt om het voorkomen van ontbrekende waarden te analyseren (_MICE_ staat voor _Multiple Imputation by Chained Equations_, verwijzend naar een techniek om ontbrekende waarden te schatten). 

We gaan er ook van uit dat je bekend bent met de inhoud van onze `ggplot2` en `dplyr` tutorial. Je kunt de survey[^gss] dataset laden die bij deze tutorial wordt geleverd als je zelf dingen wilt uitproberen.[^try] 

[^gss]: De `survey` data die gebruikt wordt komt van de General Social Survey, en bevat algemene informatie over sociale aspecten van de Amerikaanse burgers. In het bijzonder bevat het `survey` data.frame een steekproef van categorische attributen. 

Als je het zelf wilt proberen, volg dan stap voor stap de tutorial. Het incrementele karakter van het cleaning- en transformatieproces staat niet toe om delen van deze tutorial geïsoleerd uit te voeren.


```{r}
survey <- readRDS("survey.RDS")
glimpse(survey)
```

## Inleiding 

Deze tutorial over schoonmaken en transformeren is verdeeld in drie verschillende secties.

* Lezen 
* Schoonmaken
* Transformeren

__Data import __. De eerste taak is het inlezen van de gegevens in een bepaald formaat in R. Hoewel we dit onderwerp niet uitgebreid zullen behandelen, geven we wel nuttige verwijzingen naar geschikte R-pakketten om dit te doen.

__Data cleaning__. We _cleanen_ de gegevens door: fouten, duplicaten, enz. te verwijderen. 

__Data Transformation__. Hier richten we ons niet op het verwijderen van fouten en inconsistenties, maar proberen we de gegevens gemakkelijker analyseerbaar te maken: discrete representaties maken van continue variabelen, berekende variabelen toevoegen, of de labels van categorische variabelen hercoderen. 

Deze stappen vinden in het algemeen plaats voordat we enige analyse of visualisatie doen die we in de ggplot2 en dplyr tutorials hebben gezien, hoewel vaak meerdere iteraties nodig zijn. Tot nu toe ontvingen we onze gegevens altijd in een vrij schone staat, maar dat is zelden het geval in de werkelijkheid. Nu is het tijd om onze eigen cleaning te doen. Laten we aan de slag gaan!


## Gegevens lezen

Voor dit vak zullen we de verschillende data formaten en hoe ze te lezen niet uitvoerig bespreken. [^import] Meestal zullen we de `readRDS` functie gebruiken, die je waarschijnlijk al eerder hebt gezien. Bijvoorbeeld,


[^import]: Het gedeelte over het lezen van gegevens moet worden gezien als achtergrondmateriaal voor als je het nodig hebt. Er wordt alleen van je verwacht dat je bekend bent met de functies en formaten die in de lessen worden besproken. Als je echter in je toekomstige (studenten)loopbaan een bepaald gegevensbestand moet importeren, kan je dit als uitgangspunt gebruiken. De inhoud van dit deel kan dus gedeeltelijk worden overgeslagen, __uitgezonderd__ voor het deel over het converteren van variabelen.


```{r}
survey <- readRDS("survey.RDS")
```

Een .RDS-bestand slaat een __enkel__ R-object op een geserialiseerde manier op. (RDS kan worden opgevat als R Data Serialized). We kunnen een .RDS-bestand maken met `saveRDS` en er een lezen met `readRDS`.


Informatiesystemen zullen __nooit__ gegevens exporteren als .RDS-bestanden -- alle .RDS-bestanden worden gemaakt binnen R. Alle .RDS-bestanden die je hebt gebruikt in de oefeningen en tutorials zijn door ons gemaakt. Dus, in welk type bestanden kunnen gegevens in het wild dan gevonden worden? We geven je een korte rondleiding langs veel voorkomende bestandsformaten.

### CSV and TSV

CSV-bestanden zijn waarschijnlijk het meest voorkomende type gegevensbestanden. CSV staat voor Comma Seperated Values (komma gescheiden waarden). Deze bestanden kunnen worden gezien als gewone tekstbestanden, waarbij elke regel een waarneming is, d.w.z. een rij, en de kolommen worden gescheiden door komma's (vandaar de naam). De eerste rij kan de namen van de kolommen bevatten, hoewel dit niet noodzakelijk is. TSV is een veel minder gebruikelijke variant, die staat voor Tab Separated Values. Zoals de naam al zegt, worden de waarden in dit bestand niet gescheiden door komma's, maar door tabs. 

Voor CSV-bestanden zijn er twee verschillende importfuncties: `read.csv` en `read.csv2`. De eerste is voor normale door komma's gescheiden bestanden, terwijl de tweede voor door puntkomma's gescheiden bestanden is. Verder is het gebruik vergelijkbaar met `readRDS`. De functies voor TSV zijn vergelijkbaar -- alleen met een T in plaats van een C.


```{r eval = F}
data <- read.csv("path/to/data/file.csv")

data <- read.csv2("path/to/data/file.csv")
```

Beide functies zijn base-R functies, en hebben veel extra argumenten om het resulterende data.frame te verfijnen op basis van eigenaardigheden in het databestand. Ze worden echter minder gebruikt sinds het `readr` pakket uit de tidyverse snellere functies introduceerde met betere defaults. Deze functies zijn `read_csv` en `read_csv2`, d.w.z. met een underscore in plaats van een punt.

### Excel

Hoewel wij niet erg van Excel houden, doen veel mensen dat helaas nog steeds. Het is dan ook waarschijnlijk dat je vroeg of laat een Excel bestand moet inlezen. Het inlezen van Excel bestanden kan met behulp van het speciale `readxl` package. Dit pakket bevat de `read_excel` functie. 

```{r eval = F}
library(readxl)
data <- read_excel("path/to/excel/file.xlsx")
```


Nogmaals, net als voor csv, zijn er veel extra argumenten in `read_excel`. Je kan bijvoorbeeld het blad in het excel-bestand dat je wilt lezen instellen, je kunt de types van de variabelen configureren, en je kunt zelfs een bereik in het excel-bestand opgeven dat je wilt lezen, bijv. B3:G8. 

### JSON en XML

JSON - of JavaScript Object Notation - en XML - eXtensible Markup Language - zijn veel complexere gegevensnotaties in vergelijking met CSV. We zullen deze formaten hier niet bespreken, maar in plaats daarvan alleen de pakketten noemen die je kunt gebruiken als je deze types tegenkomt.

* Voor JSON is het meest gebruikte R-pakket `jsonlite`, dat de `fromJSON` functie bevat.
* Voor XML zijn er meerdere opties, maar wij adviseren het `xml2` pakket. Voor `xml` bestanden is er niet een enkele functie, maar zul je typisch veel functies moeten combineren om de data in het juiste formaat te krijgen.

### Andere statistische pakketten

Soms moet je gegevens lezen die afkomstig zijn van andere commerciële data-analyse en statistische software die door minder R-vaardige medewerkers wordt gebruikt. (Vaak is dit nodig omdat de betreffende analyse niet door de commerciële pakketten kan worden gedaan en R je moet redden.) In het bijzonder kunnen bestanden afkomstig zijn van SPSS, STATA en SAS. Voor elk van deze bestanden bevat het `haven` pakket een lees-functie. 

```{r eval = F}
# SPSS
read_spps("file")

# Stata 
read_dta("file")

#sas
read_sas("file")
```

### Databases

Tenslotte is het ook mogelijk gegevens te analyseren die in een database zijn opgeslagen. De manier waarop zal afhangen van het type database. Een van de handige pakketten is `DBI`, maar je hebt dan wel een specifieke database backend nodig, zoals `RMySL`, `RSQLite`, `RPostgreSQL`). Ook handig is `dbplyr`, waarmee veel `dplyr` functies direct op een databank gebruikt kunnen worden, zodat zware berekeningen niet door je pc gedaan hoeven te worden. 

### Achtergrond materiaal

Meer informatie over het importeren van gegevens vindt u in hoofdstuk 11 van het [R for Data Science boek](https://r4ds.had.co.nz/data-import.html), en op de helppagina's van genoemde pakketten en functies. 

_**Dit is het einde van de optionele sectie over data import**_


### Variabelen omzetten

Vaak is een integraal deel van het lezen van gegevens uit bestanden, ervoor te zorgen dat alle variabelen in onze gegevens correct zijn opgeslagen. Laat factoren factoren zijn, en getallen getallen. Laten we eens kijken naar de dataset.

```{r}
glimpse(survey)
```

Dat lijkt niet erg juist. Door een of andere duivelse kracht zijn alle variabelen als characters opgeslagen, wat niet echt correct is. De variabelen `jaar` en `leeftijd` moeten zeker numeriek zijn, terwijl `huwelijk` bijvoorbeeld duidelijk een nominale variabele is, en deze als factor moet worden opgeslagen. 

Het type van variabelen kan veranderd worden met de volgende functies: [^convert]

* `as.numeric` -> voor numerieke variabelen
* `as.integer` -> voor gehele getallen variabelen
* `(as.)factor` -> voor nominale variabelen
* `(as.)ordered` -> voor ordinale variabelen
* `as.character` -> voor karaktervariabelen

[^convert]: Merk op dat deze conversies niet standaard zonder gevaar zijn. Een variabele kan bijvoorbeeld alleen numeriek worden gemaakt als alle waarden ervan als numerieke waarden kunnen worden behandeld. Indien het waarden vindt die niet correct kunnen worden omgezet, zoals tekst, zal het in plaats daarvan een ontbrekende waarde invoegen (NA, voor Not Available, zoals we hieronder zullen zien). Het invoegen van NA's zal altijd leiden tot een waarschuwing. Zo'n waarschuwing zal je meestal waarschuwen dat je iets verkeerd deed (een verkeerde variabele geconverteerd?) of dat er fouten in de gegevens zitten.


Om dit op te lossen, gebruiken we een oude bekende uit `dplyr`: `mutate`. We hebben al geleerd dat mutate gebruikt kan worden om nieuwe variabelen aan een dataset toe te voegen, maar we kunnen het net zo goed gebruiken om bestaande variabelen te _overschrijven_. 

```{r}
survey %>%
	mutate(year = as.integer(year),
		   marital = as.factor(marital),
		   age = as.integer(age),
		   race = as.factor(race),
		   rincome = as.factor(rincome),
		   partyid = as.factor(partyid),
		   relig = as.factor(relig),
		   denom = as.factor(denom),
		   tvhours = as.numeric(tvhours)) %>%
	glimpse	
```

Dat ziet er al beter uit! Merk echter op dat we het resultaat van onze inspanningen nog niet hebben opgeslagen. Eigenlijk willen we van de gelegenheid gebruik maken om alle variabelen een makkelijke en begrijpelijke naam te geven. Hiervoor kunnen we de `rename` functie gebruiken. `rename` is een dplyr functie met een heel duidelijke taak: variabelen hernoemen. Je kunt hem gebruiken door hem een lijst met nieuwe namen te geven, gekoppeld aan oude namen: `new_name = old_name`. 

```{r}
survey %>%
	mutate(year = as.integer(year),
		   marital = as.factor(marital),
		   age = as.integer(age),
		   race = as.factor(race),
		   rincome = as.factor(rincome),
		   partyid = as.factor(partyid),
		   relig = as.factor(relig),
		   denom = as.factor(denom),
		   tvhours = as.numeric(tvhours)) %>%
	rename(reported_income = rincome,
		   party = partyid,
		   religion = relig,
		   denomination = denom,
		   tv_hours = tvhours) %>%
	glimpse	
```

Er is zeker geen juist antwoord voor de naamgeving van variabelen. Zorg er gewoon voor dat hun namen begrijpelijk, gemakkelijk te gebruiken en enigszins uniform getypt zijn. 

Merk verder op dat wat we net deden niet de enige mogelijke manier is. We zouden bijvoorbeeld ook direct de nieuwe variabelennamen kunnen maken met mutate, hoewel we daarna de oude namen wel nog zullen moeten verwijderen. 

```{r}
survey %>%
	mutate(year = as.integer(year),
		   marital = as.factor(marital),
		   age = as.integer(age),
		   race = as.factor(race),
		   reported_income = as.factor(rincome),
		   party = as.factor(partyid),
		   religion = as.factor(relig),
		   denomination = as.factor(denom),
		   tv_hours = as.numeric(tvhours)) %>%
	select(-rincome:-tvhours) %>%
	glimpse	
```

Het resultaat is hetzelfde, maar de code is iets korter. Als je dit echt onder de knie wilt krijgen, is het misschien interessant om te weten dat er veel varianten op mutate zijn die je leven misschien nog eenvoudiger (of verwarrender) maken.

* transmute: dit zal __alleen__ de _nieuwe_ variabelen in je lijst houden
* mutate_if: dit werkt op dezelfde manier als select_if, b.v. een functie toepassen op een bepaald type kolommen
* mutate_at: dit zal een functie toepassen op een bepaalde set van kolommen die je opgeeft. 

Maak je geen zorgen. Je zult een heel eind komen als je select, mutate en rename kunt gebruiken. Maar wees niet bang om jezelf uit te dagen en de meer geavanceerde dingen uit te proberen. 

Nu, laten we verder gaan. Voordat we dat doen, kopiëren we het laatste stukje code, waarbij we dit keer het resultaat weer opslaan als `survey`, waarmee we de oude versie overschrijven. U kunt dit op twee manieren doen: of je zet `survey <-` voor het stukje code, of je zet ` -> survey` na het stukje code. Nogmaals, er is geen foute of goede manier. Persoonlijk geef ik de voorkeur aan de laatste optie, omdat het mooi aansluit bij ons verhaal dat we hebben gemaakt met het `%>%` symbool: we nemen een dataset, we voeren een aantal stappen uit, en dan slaan we het op. 

```{r}
survey %>%
	mutate(year = as.integer(year),
		   marital = as.factor(marital),
		   age = as.integer(age),
		   race = as.factor(race),
		   reported_income = as.factor(rincome),
		   party = as.factor(partyid),
		   religion = as.factor(relig),
		   denomination = as.factor(denom),
		   tv_hours = as.numeric(tvhours)) %>%
	select(-rincome:-tvhours) -> survey
```

Dit is een goede plaats om aandacht te besteden aan work-flow aspecten. Vroeger, tijdens de analyse van gegevens, hingen verschillende stukken code zelden van elkaar af. Als we bijvoorbeeld grafiek A maakten en daarna tabel B, konden beide onafhankelijk van elkaar worden gemaakt. We sloegen de resultaten die we maakten nooit op om later te gebruiken (afgezien van een steekproef van gegevens die we soms namen). Nu we echter de gegevens gaan opschonen en transformeren, zullen we het data.frame altijd bijwerken, meestal onder dezelfde naam. We willen immers niet eindigen met een lijst van survey, survey2, survey3, survey4, zonder hun verschillen te onthouden. Dus, bij elke stap, updaten we de vorige versie van de survey dataset.

Er is echter een risico. Als wij een fout maken, kunnen onze gegevens worden beschadig. Als wij bijvoorbeeld race perongeluk hebben geconverteerd naar numeriek, zal de functie as.numeric hier niet in slagen en in plaats daarvan een kolom vol met NA's creëren. We kunnen dan snel onze fout in de code rechtzetten, maar dit zal de originele race variabele niet terugbrengen -- die was weg op het moment dat we ze per vergissing converteerden. 

Om onze fouten recht te zetten, zullen we de gegevens opnieuw moeten laden, en ook alle transformaties die we al eerder hebben toegepast. Alleen de code corrigeren zal niet meer voldoende zijn, __we moeten onze data corrigeren__. Wanneer je in R Markdown werkt, kun je dit het beste doen met de centrale knop in een R-chunk, omdat je dan alle vorige R-chunks opnieuw uitvoert, waardoor onze gegevens weer in de staat komen waarin ze eerder waren. 

Deze afhankelijkheden in onze workflow betekenen ook dat oefeningen meer van elkaar afhankelijk zullen zijn, en we moeten er altijd voor zorgen dat we ons bijgewerkte data.frame opslaan. Het niet bijwerken van de gegevens (of het niet uitvoeren van de code) zal later een frequente bron van fouten zijn. Let op. (U bent gewaarschuwd)

## Cleaning Data


Nu we de gegevens hebben geïmporteerd en er zeker van zijn dat alle variabelen ten minste het juiste type hebben, is het tijd om de gegevens op te schonen. In het bijzonder zullen we de volgende onderwerpen behandelen

* Dubbele observaties
* Cleanen van categorische variabelen
* Cleanen van continue variabelen
* Controleren van inconsistencies in de gegevens

Verder zullen we ook enige tijd besteden aan het bespreken van ontbrekende waarden. Dat zijn een heleboel concepten om te behandelen, dus laten we beginnen!

### Dubbele observaties

Soms kan het gebeuren dat sommige rijen per ongeluk meerdere keren in de dataset zijn opgenomen. Er is een gemakkelijke manier om deze te vinden, en te verwijderen. 
De `duplicated` functie (een base-R functie), geeft een logische vector terug die dubbele rijen in een dataset aangeeft. De vector heeft dezelfde lengte als het aantal rijen in de dataset, en is `TRUE` voor rijen die niet uniek zijn, en `FALSE` anders.

```{r}
survey %>%
	duplicated %>%
	summary
```
Het lijkt erop dat er 263 in onze gegevens zitten die niet uniek zijn. We kunnen deze bekijken door de uitvoer van duplicated te gebruiken als invoer van filter. 


```{r}
survey %>%
	filter(duplicated(.))
```

Zie je de . in de `duplicated` functie? De punt heeft een speciale betekenis als hij samen met het piping-symbool wordt gebruikt. Intern zal het vervangen worden door de invoer die door het piping symbool komt. Als zodanig is `survey %>% filter(duplicated(.))` gelijk aan `filter(survey, duplicated(survey))`. Dit is erg handig als je meerdere keren naar de piping-invoer moet verwijzen, niet alleen als eerste argument van de functie.

Op dit moment hebben we de dubbele rijen geselecteerd en kunnen we ze bekijken. Als we alleen de unieke rijen willen behouden, kunnen we een !-symbool toevoegen om de selectie te negeren.

```{r}
survey %>%
	filter(!duplicated(.))
```

Dit is echter een beetje omslachtig. Daarom bevat `dplyr` een heel handige short cut: de `distinct` functie.

```{r}
survey %>%
	distinct()
```

Of we dubbele rijen willen verwijderen of niet hangt echt af van de gegevens en context. In ons geval is het helemaal niet verwonderlijk dat sommige van deze rijen dezelfde zijn. Het is gewoon zo dat sommige mensen erg op elkaar lijken: dezelfde leeftijd, inkomen, religie enz. 

Maar in andere gevallen zouden dergelijke dubbele rijen onmogelijk zijn. Bijvoorbeeld, als er variabelen zijn die elke rij per definitie uniek zouden maken, zoals een nationaal identificatienummer. In dergelijke gevallen moeten dubbele rijen duidelijk verder bekeken worden en kan het verwijderen ervan de juiste oplossing zijn. 

### Cleaning categorische variabelen

Voor het opschonen van categorische variabelen bespreken wij de volgende wijzigingen

* Waarden hercoderen
* Herordenen van waarden

#### Hercoderen Categorische Variablen

Soms hebben categorische variabelen, d.w.z. factoren, vreemde of zelfs verkeerde labels. In dat geval zouden we deze waarden willen _hercoderen_. Het vinden van verkeerde labels is niet altijd gemakkelijk, en vaak komen deze fouten later tijdens de analyse aan het licht, in welk geval je een stap terug moet doen en ze achteraf moet corrigeren. Niettemin kan het bekijken van frequentietabellen in alfabetische volgorde, of gerangschikt van minst naar meest frequent, wijzen op enkele fouten. [^count] Laten we de partijvariabele als voorbeeld nemen. 

[^count]: De `count` functie die hier gebruikt wordt is een `dplyr` afkorting voor `group_by(party) %>% summarize(n = n())`. Voel je vrij om het te gebruiken om je een hoop typewerk te besparen. Het heeft ook een sort argument om te sorteren op aflopende frequenties. Als zodanig is `count(party, sort = T)` gelijk aan `group_by(party) %>% summarize(n = n()) %>% arrange(-n)`. Let er echter op dat `count` de hele groepering van een data.frame achteraf verwijdert, in tegenstelling tot `summarize`.


```{r}
survey %>%
	count(party) 
```

Hoewel de waarden voor partij niet echt fout zijn, zijn ze niet allemaal uniform: Str en Strong, Ind en Independent... Laten we ze veranderen. We kunnen de factor labels hercoderen met de `fct_recode` functie van forcats. Als argumenten moeten we opgeven welke variabele we willen hercoderen, en welke levels we willen veranderen. 

```{r eval = F}
data %>%
	mutate(<factor_name> = fct_recode(<factor_name>, 
									  "<new_level1>" = "<old_level1>",
									  "<new_level2>" = "<old_level2>")
```

We kunnen zoveel oude velden hercoderen in nieuwe velden als we willen. Bovendien kun je meerdere oude velden vervangen door hetzelfde nieuwe veld. Elk niveau dat niet genoemd wordt, zal ongewijzigd blijven. Laten we wat uniformiteit brengen in de politieke voorkeuren. 


```{r}
survey %>%
	mutate(party = fct_recode(party,
								"Republican, strong"    = "Strong republican",
								"Republican, weak"      = "Not str republican",
								"Independent, near rep" = "Ind,near rep",
								"Independent, near dem" = "Ind,near dem",
								"Democrat, weak"        = "Not str democrat",
								"Democrat, strong"      = "Strong democrat"
	)) -> survey 
```

Vergeet niet de dataset op te slaan!

#### Ordenen Categorische Variablen

Een andere mogelijkheid, vooral voor ordinale factoren, is dat de waarden niet echt fout zijn, maar dat ze in de verkeerde volgorde staan. Normaal is dit iets waar we opletten bij het lezen van de data, maar soms wel eens vergeten. Kijk bijvoorbeeld eens naar het gerapporteerde inkomen.

```{r}
survey %>%
	count(reported_income)
```

De waarde "Lt \$1000" - wat betekent Limited, of minder dan \$1000 - zou eerst moeten worden getoond, maar in plaats daarvan staat het op de verkeerde plaats. Hier hebben we een andere forcats functie nodig, namelijk `fct_relevel`. Deze functie kan op twee verschillende manieren gebruikt worden om een level op een andere plaats te zetten.

__Optie 1__: Verplaats één (of meer) niveau('s) naar voren

```{r eval = F}
data %>%
	mutate(factor_name = fct_relevel(factor_name, 
									 "level1_to_move","level2_to move", "..."))
```

__Optie 2__: Een (of meer) niveau('s) invoegen na een aantal N van niveaus

```{r eval = F}
data %>%
	mutate(factor_name = fct_relevel(factor_name, 
									 "level1_to_move","level2_to move", "...", after = N))
```

Dus, laten we het LT $1000 niveau naar de eerste plaats verplaatsen.

```{r}
survey %>%
	mutate(reported_income = fct_relevel(reported_income, 
										 "Lt $1000")) -> survey
```

We kunnen de resultaten controleren door telling te gebruiken op het bijgewerkte `survey` data.frame. (Want je hebt je wijziging opgeslaan, toch?)

```{r}
survey %>% 
	count(reported_income)
```

Het wijzigen van de volgorde van de niveaus van een categorische variabele is nuttig voor zowel nominale als ordinale gegevens. Voor ordinale gegevens is het logisch dat we willen dat de volgorde van de niveaus de juiste is. Maar ook voor nominale gegevens kan het nodig zijn de volgorde te veranderen. Er zijn bijvoorbeeld vaak "catch-all"-waarden zoals "Andere" of "Diverse". Het is een goede gewoonte om deze waarden anders te behandelen dan de gewone waarden in een nominale variabele, door ze als laatste te zetten. Ze komen dan aan de ene kant van een grafiek of tabel te staan, en niet tussen de andere waarden. Laten we eens kijken naar partij.


```{r}
survey %>%
	count(party)
```

Beslissen of een factor ordinaal is of niet, is niet altijd zo eenvoudig. Als we naar het gerapporteerde inkomen kijken, is het duidelijk dat er een rangorde is. Maar we hebben `partij` niet gedefinieerd als een geordende factor. Er is geen "beste" of "superieure" politieke partij, dus het expliciet programmeren van deze variabele als een ordinale factor zou een brug te ver zijn -- we zouden dan moeten beslissen welke kant van het politieke spectrum de "laagste" en welke de "hoogste" is. Dit is echter enigszins ongewenst als we grafieken maken.


```{r}
survey %>%
	ggplot(aes(party)) +
	geom_bar(fill = "dodgerblue4") + 
	theme_light() +
	theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

```

Wanneer we geen ordinale factor hebben, zal de volgorde van de labels vaak alfabetisch zijn. In dit geval, omdat we de labels eerder hebben gehercodeerd, staan ze niet eens meer in alfabetische volgorde [^order]. De resulterende grafiek is moeilijk te lezen, omdat de x-as door elkaar is geschud. Een natuurlijke reflex zou zijn om het staafdiagram te ordenen volgens frequentie, maar dat zou de leesbaarheid in dit speciale geval niet echt verbeteren. In plaats daarvan kunnen we een logischer volgorde toepassen, zonder dat we de partij als een ordinale variabele hoeven te beschouwen. Een dergelijke logische volgorde is gemakkelijk beschikbaar voor de huidige variabele, aangezien we vaak spreken van linkse en rechtse politici. We kunnen de alternatieve antwoorden (Weet niet, Geen Antwoord, Andere Partij) laten staan, hetzij aan het begin of aan het einde van de volgorde. Door ze niet te vermelden in de code hieronder, zal het laatste gebeuren.

[^order]: Toen we voor het eerst as.factor(party) deden, werden de niveaus in alfabetische volgorde geplaatst. Daarna hebben we echter sommige niveaus gehercodeerd, maar dit heeft de volgorde niet bijgewerkt. Zo werd "Strong Republican" nu "Republican, Strong" en "Strong Democrat" werd "Democrat, Strong", maar beide bleven de laatste niveaus omdat ze oorspronkelijk begonnen met S. Het is niet te verwachten dat je bekend bent met alle neveneffecten van sommige transformaties, maar dit toont je de complexiteit wanneer onze acties beginnen af te hangen van eerdere acties, en hoe we echt werken in een iteratieve context.

```{r}
survey %>%
	mutate(party = fct_relevel(party, 
							   "Democrat, strong",
							   "Democrat, weak", 
							   "Independent, near dem",
							   "Independent", 
							   "Independent, near rep",
							   "Republican, weak",
							   "Republican, strong")) -> survey
```

Onze grafiek ziet er nu als volgt uit. Beter, is het niet? [^beter]

[^better]: _Beter_ is natuurlijk subjectief en hangt af van wat je jouw publiek wilt laten zien. Als de gegevens over Vlaamse politieke partijen zouden gaan, is het moeilijker om een logische volgorde aan te brengen. (B.v. wie is meer "links", Groen of Sp.a?) Omdat er niet dezelfde nuances in deze partijen zitten, zoals "zwak" of "sterk", zou het logischer zijn om ze te rangschikken volgens grootte (wat wel gebeurt bij Vlaamse verkiezingen, maar niet noodzakelijk bij verkiezingen in de VS). Uiteindelijk is geen enkele situatie hetzelfde en wordt veel overgelaten aan jouw beslissingen en veronderstellingen als data-analist. 

```{r}
survey %>%
	ggplot(aes(party)) +
	geom_bar(fill = "dodgerblue4") + 
	theme_light() +
	theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```


In conclusie: gebruiken we

* `fct_recode` voor het hercoderen van waarden van een categorische variabele, en
* `fct_relevel` voor het handmatig herordenen van waarden van een categorische variabele. 

Later zullen we meer specifieke functies zien voor het hercoderen en herordenen van categorische variabelen bij het transformeren van gegevens (niet omdat er fouten zijn, maar omdat ze onze analyse daarmee kunnen verbeteren). Zorg ervoor dat je het overzicht niet verliest! Eerst kijken we naar het opschonen van continue gegevens. 


### Cleaning continue variabelen

Voor continue gegevens is het bereik van mogelijke waarden oneindig, en is het dus moeilijker _foute_ waarden te vinden. Zonder informatie over de context van de gegevens is het vinden van foute continue waarden uiterst moeilijk. 

#### Fouten

In het `survey` data.frame, zijn er drie continue variabelen: jaar, leeftijd en (dagelijkse) tv_hours. Voor elk van deze variabelen hebben we een idee over het mogelijke bereik van waarden. De leeftijd zal waarschijnlijk ergens tussen 20 en 100 liggen (wetende dat de dataset informatie over volwassenen bevat). Het aantal tv-uren moet tussen 0 en 24 liggen, aangezien er 24 uur in een dag zitten. Ook voor het jaar weten we min of meer wat we kunnen verwachten. Laten we eens kijken naar elk. [^cont]

```{r}
summary(survey$year)
```

Voor het jaar, lijkt alles in orde. We hebben het al eerder omgezet in een gehele variabele, dus we hoeven niet te controleren op foutieve decimale jaren. Ook het minimum en maximum lijken in orde. Als het jaar fouten bevat, zien we dat waarschijnlijk in deze extremen: 2102 in plaats van 2012, 1099 in plaats van 1999, of bv. 9999 wat aangeeft dat het eigenlijk ontbreekt.

[^cont]: Wanneer we categorische gegevens inspecteren, kunnen we een snelle telling doen. Hoewel dit zou kunnen werken voor sommige continue variabelen, zoals jaar, is het niet geschikt voor de meeste continue variabelen, omdat ze vaak uniek zijn voor elke observatie. In plaats daarvan kijken we naar de samenvatting van de variabelen.

Laten we dan eens kijken naar leeftijd.

```{r}
summary(survey$age)
```

Op het eerste gezicht lijken er ook geen problemen te zijn met de leeftijd. Er zijn 76 ontbrekende waarden, maar de aanwezige waarden liggen tussen 18 en 89 jaar, wat ook weer een logisch bereik is. Ook leeftijd werd eerder zonder problemen omgezet in een gehele variabele, zodat alle waarden gehele getallen zijn.

Als we naar de tv-uren kijken, zien we iets eigenaardigs.

```{r}
summary(survey$tv_hours)
```

We zien dat er 10146 ontbrekende waarden zijn, wat hoog is maar niet noodzakelijk verkeerd (tenzij we per ongeluk enkele waarden hebben verwijderd, wat we niet hebben gedaan). Er zijn geen negatieve waarden, aangezien het minimumaantal uren dat iemand tv heeft gekeken nul is. Aan de andere kant zien we echter dat het aantal tv-uren oploopt tot 84 uur per dag - dit is duidelijk verkeerd. We hebben allemaal maar 24 uur in een dag. 

We kunnen verder kijken naar de records waarvoor de tv-uren meer dan 24 zijn.

```{r}
survey %>%
	filter(tv_hours > 24)
```

Er blijken 5 waarnemingen te zijn waarvoor het aantal tv-uren duidelijk fout is, en die moeten we corrigeren. We hebben echter geen idee hoe we dat in ons geval moeten doen. Het enige wat we kunnen doen, is deze waarden schrappen, en ze missing maken.[^validation] Let op, we verwijderen niet de volledige waarnemingen, alleen de tvhours variabele voor deze waarnemingen. De andere variabelen kunnen nog steeds worden gebruikt voor deze 5 rijen. 

In sommige gevallen kan je proberen te achterhalen wat de fout is. Als het gaat over de lengte van personen zijn er mogelijke enkele waarden in meter uitgedrukt, en de rest in centimeter? Misschien is er sprake van verschillende eenheden (meter vs feet, km vs mile). Anders kan je in werkelijkheid ook proberen teruggaan naar de bron van de data, of de domein expert, om te achterhalen wat er mis is. Als al deze acties geen opheldering bieden, zit er niets anders op dan de waarden te verwijderen. 

[^validation]: Dergelijke fouten kunnen vaak worden vermeden door een goede gegevensverzameling, d.w.z. als je online een enquête of een gegevensformulier maakt, zorg er dan voor dat alle velden redelijk gecontroleerd zijn: leeftijd kan niet negatief zijn, een postcode bestaat uit 4 cijfers, en men heeft niet meer dan 24 uur in een dag. Hoewel er later nog dingen mis kunnen gaan, kunnen de gegevens in ieder geval niet verkeerd worden ingevoerd door de respondenten. In het echte leven gebeurt veel gegevensverzameling helaas zonder veel nadenken. 

We kunnen dit doen door gebruik te maken van de `ifelse` functie. Deze functie is een zeer algemene functie die een waarde teruggeeft die afhankelijk is van een logische test.[^ifelse] De functie kan als volgt worden gebruikt.

[^ifelse]: Misschien ben je bekend met de `IF` functie in Excel? Het gebruik ervan is precies hetzelfde. 

```{r eval = F}
if_else( <condition>, <value_a>, <value_b> )
```

Stel, we hebben een vector `score` die leerlingenscores bevat. We kunnen de `ifelse` functie gebruiken om een vector `grade` te maken met de waarden `FAILED` en `PASSED`.

```{r eval = F}
grade <- ifelse(score >= 10, "PASSED", "FAILED")
```

In onze situaties is de vector die we willen aanpassen een kolom in een dataframe, dus we gebruiken mutate om dit te doen. Laten we de functie gebruiken om de variabele tv-uren bij te werken.

```{r}
survey %>%
	mutate(tv_hours = ifelse(tv_hours > 24, NA, tv_hours)) -> survey
```

Dus, wat gebeurt er? De variabele tv_hours wordt bijgewerkt met `mutate`. Als deze groter is dan 24, is de nieuwe waarde `NA`, d.w.z. Not Available, missing. Anders is de nieuwe waarde gewoon de oude waarde van tv_hours, d.w.z. ongewijzigd. Nadat de kolom is bijgewerkt, wordt het nieuwe data.frame weer opgeslagen als `survey`.

Het controleren op fouten, zowel categorische als continue, kan een _straat zonder einde_ zijn. Meestal doe je je uiterste best door voor elke variabele de hierboven besproken testen toe te passen tijdens je beschrijvende analyse. Als dit veel werk lijkt, bedenk dan dat een doorsnee dataproject voor 70% bestaat uit het opschonen en transformeren van gegevens, en voor slechts 30% uit de eigenlijke analyse en interpretatie. 


En zelfs dan kan het gebeuren dat je, ondanks al je tijd en inspanningen, in de analysefase fouten in de gegevens ontdekt. Dat is niet verwonderlijk, want op dat moment ga je de gegevens echt in detail bekijken. Wanneer dit gebeurt, ga je terug naar uw cleaning- en transformatiescripts en pas je ze aan waar nodig. Het is dus belangrijk dat je zowel de oorspronkelijke gegevens als alle wijzigingen die je hebt aangebracht, bewaart. Nog belangrijker is het om ervoor te zorgen dat al je analyses worden opgeslagen als een script van RMarkdown, zodat ze snel opnieuw kunnen worden uitgevoerd na het corrigeren van de fout. Dit heet _reproducible research_ en zal je veel tijd en fouten besparen. Kijk eens naar [deze video](https://www.youtube.com/watch?v=s3JldKoA0zw) voor een illustratie van reproduceerbare work-flows. 


#### Outliers


Het komt ook voor dat sommige continue waarden niet noodzakelijk fout zijn, maar _uitzonderlijk_ hoog of laag. Deze uitzonderingen noemen we geen fouten, maar outliers -- een gegevenspunt dat ver af ligt van andere waarnemingen. Neem bijvoorbeeld tv-uren. Eerder verwijderden we de waarden groter dan 24 uur, omdat daar logischerwijze fouten in zitten. Laten we eens kijken naar de resterende waarden. 

```{r}
survey %>%
	ggplot(aes(tv_hours)) +
	geom_histogram(binwidth = 1,
				   color = "white", fill = "dodgerblue4") +
	theme_light()
```

Het lijkt erop dat, hoewel de meeste mensen tussen 0 en 5 uur per dag televisie kijken, er enkele uitzonderlijk hoge waarden zijn. Het is niet duidelijk of het hier gaat om vergissingen zoals we die eerder hebben verwijderd, of gewoon om abnormale tv-verslaafden. Laten we eens kijken naar de 25 hoogste waarden, en ze vergelijken met enkele verwante attributen, zoals leeftijd, inkomen en burgerlijke staat (ze vergelijken met godsdienst of partij zou kunnen worden beschouwd als politiek incorrect, dus laten we uit die gevarenzone weg blijven). [^pander]

[^pander]: Merk op dat we pander gebruiken om de opmaak van onze tabellen te verbeteren, en dat je daar verder geen aandacht aan moet besteden in deze tutorial. 

```{r}
survey %>%
	arrange(-tv_hours) %>%
	slice(1:25) %>%
	select(tv_hours, marital, age, reported_income) %>%
	pander()
```

Het lijkt erop dat veel van de respondenten elke dag het maximumaantal van 24 uur tv kijken, wat verrassend is. We kunnen de andere variabelen gebruiken om een beter beeld te krijgen van deze waarnemingen. We zien dat velen van hen alleenstaand zijn (nooit getrouwd, gescheiden of gescheiden). De leeftijd lijkt relatief hoog te zijn, maar niet erg opmerkelijk (de gemiddelde leeftijd voor de dataset was ongeveer 47 jaar). Ten slotte hebben velen hun inkomen niet gerapporteerd. 

Deze informatie kan op verschillende manieren geïnterpreteerd worden:

* Dit zijn alleenstaande, luie mensen met een inkomen die de hele dag tv kijken
* Dit zijn mensen die niet erg eerlijk waren bij het invullen van hun informatie (ook geen inkomen opgeven). 

Beslissen of iets fout of uitzonderlijk is, is niet triviaal en vereist bepaalde domeinkennis en aannames. In dit geval is de logische aanname dat deze cijfers onjuist zijn (zelfs luie, tv-verslaafde mensen moeten af en toe slapen). De moeilijkere vraag is op welk punt iets onjuist wordt. 20 uur? 16? Voor continue variabelen kan het helpen om naar een boxplot te kijken en te zien tot waar de whiskers gaan. Dit is echter geen exacte wetenschap, vooral wanneer variabelen niet symmetrisch verdeeld zijn. 

```{r}
survey %>%
	ggplot(aes("", tv_hours)) +
	geom_boxplot() 
```

Voor het huidige geval kunnen we zeggen dat de mensen die meer dan 8 uur tv kijken ofwel een onjuist getal hebben ingevuld, ofwel uitzonderlijke tv-kijkers zijn. Daarom verwijderen we alle waarden hoger dan 8.

```{r}
survey %>%
	mutate(tv_hours = ifelse(tv_hours > 8, NA, tv_hours)) -> survey
```


### Data inconsistenties

Een andere manier om op fouten te controleren is meer dan één variabele tegelijk te bekijken, en te controleren op duidelijke inconsistenties. Dit kunnen we zien als "regels" die worden overtreden. In de huidige dataset zouden we kunnen nagaan of alle gehuwde personen minstens 18 jaar oud moeten zijn. Aangezien alle waarnemingen mensen betreffen die minstens 18 jaar oud zijn, weten we dat deze regel niet geschonden is. 
Bijvoorbeeld regels in andere gevallen kunnen zijn:

* het vertrek van een vlucht moet plaatsvinden voordat deze kan aankomen. (Rekening houdend met verschillende tijdzones, uiteraard.)
* iemand wiens werkstatus "werkloos" is, kan geen gemeld inkomen hebben (tenzij rekening wordt gehouden met werkloosheidsuitkeringen). 
* enz.

M.a.w., we gebruiken onze kennis over de variabelen en hun onderlinge verhoudingen om te kijken of er opvallende observaties zijn, die we desnoods moeten aanpassen. 

Vaak zult u geen tijd hebben om alle mogelijke regels die je kunt bedenken te controleren. Bovendien zullen sommige regels gebaseerd zijn op bepaalde veronderstellingen die je moet controleren. Bijvoorbeeld, de [leeftijd waarop men kan trouwen](https://en.wikipedia.org/wiki/Marriageable_age) hangt af van het land, en kan dus lager of hoger zijn dan 18.

### Missing values

In het echte leven worden gegevens meestal geleverd met ontbrekende waarden. De waarden kunnen ontbreken aan de start, of ze kunnen ontbreken omdat we ze verwijderd hebben als outliers of verkeerde waarden. In de volgende paragrafen zullen we zien hoe we ontbrekende waarden kunnen analyseren -- bv. ontbreken ze willekeurig of niet? -- en hoe ze te behandelen tijdens je analyse. Wij zijn verplicht te zeggen dat er ook technieken zijn die kunnen worden gebruikt om ontbrekende waarden te _schatten_ op basis van de waarden voor andere attributen en andere waarnemingen met soortgelijke waarden. Dit wordt missing value _imputation_ genoemd en is een apart veld op zich. Vanwege de complexiteit ervan zullen we het er hier niet over hebben, maar de geïnteresseerde lezer wordt verwezen naar [deze handleiding van het mice-pakket](https://datascienceplus.com/imputing-missing-data-with-r-mice-package/).

#### Analyseren missing data


Er zijn drie verschillende manieren waarop ontbrekende gegevens kunnen voorkomen (zie theorie). 

* Volledig willekeurig ontbrekend (MCAR)
* Willekeurig ontbrekend (MAR)
* Niet willekeurig ontbrekend (NMAR)

Hieronder illustreren we enkele technieken om de ontbrekende waarden in je gegevens te analyseren. De meest voor de hand liggende manier om na te gaan of je gegevens ontbrekende waarden bevatten is door naar de samenvatting te kijken.

```{r}
summary(survey)
```

Dit vertelt ons voor welke variabelen er ontbrekende gegevens zijn, en hoeveel. Het zegt ons echter niets over de relaties tussen ontbrekende waarden. Om naar _patronen_ van ontbrekende gegevens te kijken, kunnen we gebruik maken van de `md.pattern` functie (missing data patterns) uit het pakket `mice`. 

```{r}
md.pattern(survey)
```

De uitvoer van `md.pattern` is een beetje cryptisch, maar laten we eens wat beter kijken. Elke kolom verwijst naar een van de variabelen, zoals is aangegeven. Elke rij is een _patroon_ bestaande uit 1'en (gegevens ontbreken niet) en 0'en (gegevens ontbreken wel). De eerste rij, waarin alle variabelen een 1 hebben, is een patroon waarin geen van de variabelen ontbreekt. Dit wordt ook aangeduid door de nul in de laatste kolom. In de tweede rij wordt tv hours aangegeven met een nul, wat betekent dat voor dit patroon de variabele tv_hours ontbreekt. De laatste kolom geeft dus 1 ontbrekende waarde aan. Het laatste patroon is er een met 2 ontbrekende waarden, zoals aangegeven in de laatste kolom. Met name leeftijd en tv-uren ontbreken. 

Het getal in de eerste kolom geeft aan hoeveel waarnemingen van elk patroon er zijn. Op die manier heeft het eerste patroon de meeste waarnemingen -- 10936 personen zonder ontbrekende gegevens. Het laatste patroon (tv-uren en leeftijd ontbreken) komt 38 keer voor. De laatste rij is gelijk aan het aantal ontbrekende waarden voor elke variabele (dezelfde informatie die de samenvatting ons gaf). Tenslotte is het getal in de rechter benedenhoek het totaal aantal ontbrekende waarden. 

De uitvoer van md.patterns (welke zowel een tabel als een visuele weergave is) kan ons tonen of het voorkomen van ontbrekende waarden met elkaar verband houdt. Bijvoorbeeld, als leeftijd ontbreekt, dan ontbreekt tv-uur ook? Dat laatste is niet het geval, want er zijn evenveel waarnemingen waar leeftijd ontbreekt en tv-uren niet, als waarnemingen waar leeftijd én tv-uren ontbreekt.

Naast `md.pattern` kunnen we ook nagaan of het voorkomen van ontbrekende waarden samenhangt met de _waarde_ voor andere variabelen. Zo kunnen we ons afvragen of mensen van bepaalde religies of politieke voorkeuren meer of minder kans hebben om hun leeftijd of het aantal uren dat ze tv kijken op te geven. Deze patronen kunnen gecontroleerd worden met ggplot/dplyr, door een nieuwe variabele te creëren die aangeeft of een waarneming een ontbrekende waarde heeft of niet.

Laten we eens kijken naar leeftijd. We voegen een variabele toe die aangeeft of leeftijd al dan niet ontbreekt. Merk op dat om dit te controleren, we een speciale functie nodig hebben. We kunnen age == NA niet gebruiken om te zien of leeftijd ontbreekt. In de laatste voorwaarde vergelijken we leeftijd met NA, d.w.z. we vergelijken leeftijd met iets dat we niet hebben. We kunnen nooit weten of leeftijd gelijk is aan iets dat we niet hebben, dus het resultaat daarvan is altijd NA, ongeacht de waarde van leeftijd. Neem als voorbeeld de variabelen a en b, waarvan a ontbreekt en b niet. 

```{r}
a <- NA
b <- 1

a == NA
b == NA
```

Beide logische condities geven NA terug, in plaats van de verwachte TRUE en FALSE.  

Dus, hoe controleren we of iets NA is? Daar gebruiken we de speciale functie `is.na` voor.

```{r}
is.na(a)
is.na(b)
```

Dus, voor de survey data:

```{r}
survey %>%
	mutate(age_missing = is.na(age),
		   tv_missing = is.na(tv_hours)) -> survey_md
```

Merk op dat wij het data.frame met de bijkomende variabelen onder een andere naam opslaan, aangezien wij deze variabelen in de uiteindelijke analysefase niet nodig zullen hebben, alleen om de ontbrekende gegevens voorlopig te bekijken.

Wanneer we de ontbrekende/niet-ontbrekende variabele vergelijken met een categorische variabele, hebben wij in feite 2 categorische variabelen. We kunnen dus een grafiek of tabel gebruiken die geschikt is voor het vergelijken van categorische variabelen. Zoals we nu allemaal (zouden moeten) weten, kunnen we een staafdiagram gebruiken.

```{r}
survey_md %>%
	ggplot(aes(reported_income, fill = age_missing)) +
	geom_bar(position = "fill") +
	coord_flip()
```

Uit het staafdiagram blijkt dat mensen die hun inkomen niet hebben opgegeven, vaker ook hun leeftijd niet hebben opgegeven. [^noanswer]

[^noanswer]:  Men zou kunnen aanvoeren dat we ook de waarde "Geen antwoord" voor gemeld inkomen als NA hebben gecodeerd. In dit geval is echter besloten dat niet te doen om een duidelijk onderscheid tussen de speciale categorieën (Geweigerd, Niet van toepassing, Geen antwoord en Weet niet) te behouden.

We kunnen hetzelfde doen voor tv-uren. We zien dan dat het percentage ontbrekende waarden varieert - iets hoger of lager voor bepaalde inkomens - maar dat er geen duidelijke patronen zijn. 

```{r}
survey_md %>%
	ggplot(aes(reported_income, fill = tv_missing)) +
	geom_bar(position = "fill") +
	coord_flip()
```

Natuurlijk kan het voorkomen van ontbrekende waarden worden vergeleken met meer dan één variabele, niet alleen inkomen. Hieronder tonen we de vergelijking van ontbrekende tv-uren met politieke voorkeur.  Hier zien we duidelijk dat er slechts één antwoord is voor politieke partij dat alleen werd gebruikt voor waarnemingen waarbij de tv-uren niet ontbraken. Voor de andere politieke voorkeur-waarden is er geen duidelijk verband met het missing zijn van tvhours. 


```{r}
survey_md %>%
	ggplot(aes(party, fill = tv_missing)) +
	geom_bar(position = "fill") +
	coord_flip()
```

Als laatste voorbeeld kunnen we het ontbreken van het aantal tv-uren vergelijken met een continue variabele, laten we zeggen leeftijd. Zijn mensen die het aantal uren dat ze tv kijken niet hebben gerapporteerd over het algemeen ouder of jonger? De boxplots hieronder laten geen verschil zien. 

```{r}
survey_md %>%
	ggplot(aes(tv_missing, age, color = tv_missing)) +
	geom_boxplot()
```

Hoeveel vergelijkingen u maakt voor elke variabele waarvoor gegevens ontbreken, is aan jou -- maar je moet redelijk bewijs verzamelen of de ontbrekende waarden MAR, MCAR of NMAR zijn. 

#### Werken met missing data

Hoewel we niet zullen zien hoe we ontbrekende waarden kunnen imputeren - _inschatten_ -, is het toch belangrijk te weten hoe we ermee moeten werken. 

Ten eerste, denk aan visualisaties. Wanneer we met ggplot werken, zullen ontbrekende waarden vaak automatisch genegeerd worden. Wanneer we bijvoorbeeld een histogram van de leeftijd proberen te maken, zal ggplot ons via een waarschuwing vertellen dat het enkele ontbrekende waarden negeerde. 

```{r message = T, warning = T}
survey %>%
	ggplot(aes(age)) +
	geom_histogram(binwidth = 5, fill = "dodgerblue4", color = "white") +
	theme_light()
```

De waarschuwing

```{r eval = F}
## Warning: Removed 76 rows containing non-finite values (stat_bin).
```

vertelt ons wanneer en hoeveel ontbrekende waarden worden genegeerd. 
In het geval dat een categorische variabele ontbrekende waarden heeft, zal NA verschijnen als een aparte categorie. Neem bijvoorbeeld de dataset survey2 (die we hier even als voorbeeld zullen gebruiken), die ontbrekende waarden heeft voor verscheidene variabelen.

```{r echo = F}
library(tidyr)
library(readr)
library(purrr)
survey %>%
	mutate(row = 1:n()) %>%
	gather(key, value, -row) %>%
	sample_frac(0.99) %>%
	spread(key, value) %>%
	select(-row) %>%
	mutate_all(parse_guess) %>%
	mutate_if(negate(is.numeric), as.factor) -> survey2
```

```{r}
summary(survey2)
```
Laten we zeggen dat we een staafdiagram maken van de variabele godsdienst. De waarde NA verschijnt dan als een aparte categorie en krijgt zijn eigen staaf. Automatisch zal deze categorie worden uitgezet naast de andere, niet ertussen. 


```{r}
survey2 %>%
	ggplot(aes(religion)) +
	geom_bar() +
	coord_flip()
```

Als dusdanig behandelt ggplot ontbrekende waarden zonder probleem en vrij transparant. Hetzelfde is niet waar wanneer we naar numerieke berekeningen gaan.

Laten we beginnen met het goede nieuws. Als wij een frequentietabel maken van een categorische waarde met ontbrekende waarden, zullen de NA's net zo worden beschouwd als elke andere waarde. Wij kunnen bijvoorbeeld een frequentietabel maken ter ondersteuning van de bovenstaande grafiek voor survey2. NA wordt zelfs meegesorteerd door onze arrange in dit geval. 

```{r}
survey2 %>%
	group_by(religion) %>%
	summarize(frequency = n()) %>%
	mutate(relative_frequency = frequency/sum(frequency)) %>%
	arrange(-frequency) %>% 
	pander
```

Helaas is dat niet het geval wanneer we maten voor centraliteit of spreiding gaan berekenen. Per definitie, wanneer je een functie toepast op een vector die ontbrekende waarden bevat, zal de functie een ontbrekende waarde teruggeven. Beschouw de vector hieronder, waarvan we het gemiddelde en de som willen weten

```{r}
x <- c(5, 6, 12, NA, 43)
mean(x)
sum(x)
```
Net als de waarschuwing die we kregen toen we ggplot gebruikten met ontbrekende waarden, is dit resultaat een waarschuwing. Het waarschuwt ons dat we iets willen berekenen over een vector die gedeeltelijk mist. De waarschuwing is hier echter vrij sterk; ze geeft ons niets wat we kunnen gebruiken. 


Om dit te omzeilen heeft elk van deze functies een argument dat `na.rm` heet -- wat "NA verwijderen" betekent. Als we zeggen `na.rm = T`, dan worden ontbrekende waarden verwijderd en genegeerd, en de functie berekent het resultaat met de overgebleven waarde. Dus,


```{r}
mean(x, na.rm = T)
sum(x, na.rm = T)
```

Het feit dat we expliciet moeten aangeven dat we ontbrekende waarden willen negeren is het veiligheidsmechanisme van R dat voorkomt dat we ontbrekende waarden per ongeluk negeren. 

Als we dus maten van centraliteit en spreiding willen berekenen voor, laten we zeggen, leeftijd, zullen we dit argument moeten gebruiken als we iets willen bereiken. [^easier]

[^easier]: Ja, we moeten dit herhalen voor elke functie die we willen gebruiken, en nee, er is geen gemakkelijkere manier.

```{r}
survey %>%
	summarize(min = min(age, na.rm = T), 
			  mean = mean(age, na.rm = T), 
			  max = max(age, na.rm = T), 
			  iqr = IQR(age, na.rm = T))
```

Tot slot, wat gebeurt er als we correlaties willen berekenen? Bijvoorbeeld, hoe zijn leeftijd en tv-uren gecorreleerd. Beide hebben ontbrekende waarden. We kunnen het volgende proberen:
```{r error = T}
survey %>%
	select(age, tv_hours) %>%
	cor(na.rm = T)
```


Oeps, dat was wishful thinking van mijn kant. Het lijkt erop dat het `na.rm` argument niet bestaat voor de `cor` functie. Tot zover de consistentie in de basis R functies. 

Om correlaties te berekenen, moeten we ervoor zorgen dat we alleen rijen zonder ontbrekende waarden beschouwen. We kunnen dit doen met de `na.omit` functie. Deze functie verwijdert -- omits -- alle rijen die een of meer ontbrekende waarden hebben.

```{r}
survey %>%
	select(age, tv_hours) %>%
	na.omit() %>%
	summary()
```

Dan kunnen we de correlatie berekenen.


```{r}
survey %>%
	select(age, tv_hours) %>%
	na.omit() %>%
	cor()
```

De `na.omit` functie kan wat gevaarlijk zijn en zou alleen gebruikt moeten worden in uitzonderlijke gevallen als deze. Het kan een enorm verschil maken als we het vóór select doen. Beschouw opnieuw de `survey2` dataset, waar alle variabelen een aantal ontbrekende waarden hebben.

Deze dataset heeft het volgende aantal rijen.

```{r}
survey2 %>% nrow()
```

Als we leeftijd en tv-uren selecteren, en rijen met ontbrekende waarden verwijderen, houden we het volgende aantal rijen over

```{r}
survey2 %>%
	select(age, tv_hours) %>%
	na.omit() %>%
	nrow()
```

Maar als we eerst de rijen met ontbrekende waarden verwijderen, en dan de twee variabelen selecteren, houden we slechts het volgende aantal rijen over. 

```{r}
survey2 %>%
	na.omit() %>%
	select(age, tv_hours) %>%
	nrow()
```

Zie je wat er anders is? Kijk maar eens goed. Als we de verwijdering uitvoeren voor de select, zal het rekening houden met ontbrekende waarden voor ALLE variabelen. Als we de selectie eerst doen, wordt alleen gecontroleerd op de variabelen die we behouden. Deze nuance kan in de praktijk een groot verschil maken. Het gebruik van de `na.omit` functie moet altijd een waarschuwing in je achterhoofd oproepen dat je voorzichtig moet zijn.

Het is raadzaam steeds voorzichtig te zijn bij het werken met ontbrekende waarden. En met deze wijze woorden beëindigen we onze data cleaning inspanningen, en gaan we over tot transformaties.

## Data transformatie

Terwijl het opschonen van gegevens gericht is op het vinden van fouten, gaat het bij het omvormen van gegevens om het analyseren ervan te vergemakkelijken, of resultaten te verbeteren. Er zijn vele manieren om dat te doen. 

Ten eerste kunnen we het aantal niveaus in een categorische variabele verminderen, zodat er niet te veel verschillende categorieën worden getoond. Ten tweede kunnen we ook nieuwe variabelen creëren op basis van bestaande variabelen. Dit laatste wordt ook wel het verrijken van gegevens genoemd. Bovendien kunnen we variabelen aanpassen om ze gemakkelijker te interpreteren te maken. Bijvoorbeeld door afstanden in km te gebruiken in plaats van in mijlen (of andersom als je Brits of Amerikaans bent). Verder kunnen we continue variabelen discreet maken - door ze in categorische variabelen te veranderen - zodat we andere analyses of visualisaties kunnen gebruiken. Tenslotte zullen we ook enkele transformaties bespreken die nuttig zijn bij het visualiseren van gegevens, in het bijzonder om variabelen te ordenen. 

Veel dingen te doen, dus tijd om te beginnen!

### Discretisern van continue variablen

Het omzetten van een continue variabele in een categorische variabele wordt discretisatie genoemd. Verschillende functies om dit te doen worden aangeboden door ggplot. 

* `cut_width`: intervallen van een bepaalde breedte maken
* `cut_interval`: het creëren van een specifiek aantal intervallen van gelijke breedte
* `cut_number`: het creëren van een specifiek aantal intervallen met een gelijk aantal waarnemingen. 

Bijvoorbeeld, we kunnen tv_hours discretiseren in intervallen van breedte 4. [^boundary]


[^boundary]: Het grensargument is een optioneel argument om te bepalen waar het interval moet beginnen.

```{r}
survey %>%
	mutate(tv_hours = cut_width(tv_hours, width = 4, boundary = 0)) %>%
	count(tv_hours)
```

Als alternatief kunnen we tv_-uren_hours in 4 gelijke intervallen verdelen.

```{r}
survey %>%
	mutate(tv_hours = cut_interval(tv_hours, n = 4)) %>%
	count(tv_hours)
```

Of we kunnen tv_hours in drie intervallen verdelen die een gelijk aantal waarnemingen bevatten. 

```{r}
survey %>%
	mutate(tv_hours = cut_number(tv_hours, n = 3)) %>%
	count(tv_hours)
```

Hierbij moet worden opgemerkt dat de intervallen niet helemaal evenveel waarnemingen bevatten. Dat komt omdat er veel waarnemingen zijn met een unieke waarde, die niet verder kunnen worden gesplitst. Maar toch zal cut_number zijn best doen, wat beter zal uitpakken als de waarden van de variabele unieker zijn. Bijvoorbeeld, leeftijd leent zich hier beter voor:

```{r}
survey %>% 
	mutate(age = cut_number(age, n = 5)) %>%
	count(age)
```

Elk van de discretisatie-functies staat ons toe om de namen van de categorieën aan te passen -- in plaats van de standaard interval notatie door een vector van namen mee te geven aan het `labels` argument. Bijvoorbeeld:

```{r}
survey %>% 
	mutate(age_category = cut_number(age, n = 3, labels = c("Young","Middle-aged","Old"))) %>%
	group_by(age_category) %>%
	summarize(min = min(age), max = max(age), frequency = n())
```

In het laatste voorbeeld hebben wij de gediscretiseerde variabele onder een andere naam opgeslagen. Dit is zeer aan te raden, om de oorspronkelijke, meer gedetailleerde gegevens niet te verliezen. 


### Herschalen van continue variablen

Wanneer we continue variabelen hebben, kunnen we ook de schaal aanpassen. We kunnen bijvoorbeeld het percentage tv_hours per dag berekenen door het aantal tv_hours te delen door 24.

```{r}
survey %>%
	mutate(tv_per_day = tv_hours/24) %>%
	summary
```

Andere veel voorkomende transformaties zijn:

* verschillende munteenheden (euro vs dollar, enz.)
* verschillende meetschalen (mijl vs km, inch vs cm, enz.)
* verschillende tijdzones of tijdseenheden (waarover later meer).

### Toevoegen van calculated variables


De toevoeging van berekende variabelen is vergelijkbaar met het herschalen van variabelen. Het enige verschil is dat herschalen slechts betrekking heeft op één variabele, terwijl berekende variabelen betrekking kunnen hebben op verschillende variabelen. 

Wij kunnen bijvoorbeeld het geboortejaar berekenen voor de personen in onze gegevens.

```{r}
survey %>%
	mutate(year_of_birth = year - age) %>%
	select(year, age, year_of_birth) %>%
	summary
```

### Factoren transformeren

Bij de transformatie van categorische variabelen is het vaak de bedoeling het aantal waarden te verminderen. Dit kan op verschillende manieren gebeuren:

* het combineren van waarden die op elkaar lijken tot één enkele waarde 
* het combineren van niet vaak voorkomende waarden in een "Diverse" of "Andere" waarde. 

De eerste manier kan worden gedaan met `fct_collapse`, die factor niveaus samenvouwt (collapsed) tot een nieuw niveau. De tweede manier kan worden bereikt met `fct_lump`, die niet vaak voorkomende waarden samenvoegt in één aparte groep (lump)

Het samenvouwen van een factor kan als volgt worden gedaan. Voor elke groep van niveaus die je wilt samenvouwen, maak je een vector. Vervolgens kan je elke groep een nieuwe naam geven. [^fct_recode] Alle niveaus die je niet noemt, blijven onaangeroerd.

[^fct_recode]: De oplettende lezer heeft misschien een overeenkomst opgemerkt tussen `fct_collapse` en `fct_recode`. Inderdaad, je zou hetzelfde resultaat kunnen bereiken met de laatste functie, maar je zou meer typewerk hebben. 

```{r eval = F}
fct_collapse(factor,
	new_group_1 = c("old_level_1", "old_level_2", "..."),
	new_group_2 = c("old_level_a", "old_level_b", "..."),
	...)
```

In onze survey-data kunnen we de partijvariabele in verschillende groepen indelen: "Democrat', 'Republican', 'Independent' en 'Other'. We slaan de nieuwe variabele op als party_group:

```{r}
survey %>%
	mutate(party_group = fct_collapse(party,
									  Other = c("No answer", 
									            "Don't know", 
									            "Other party"),
									  Republican = c("Republican, strong", 
									                 "Republican, weak"),
									  Independent = c("Independent, near rep", 
									                  "Independent", 
									                  "Independent, near dem"),
									  Democrat = c("Democrat, weak", 
									               "Democrat, strong")
	)) -> survey
```

Transformaties als deze kunnen op zichzelf al nuttig zijn: het aantal categorieën verminderen, zoals in deze plot.

```{r}
survey %>%
	ggplot(aes(party_group)) +
	geom_bar() +
	coord_flip()
```

Maar ze kunnen net zo goed gebruikt worden in combinatie met de originele levels. Bijvoorbeeld om de visuals te verbeteren: [^politics]

[^politics]: In dit voorbeeld gebruiken we de scale_fill_tableau kleurenschaal uit het pakket ggthemes om een kleurenschaal te krijgen waarbij de Democraten blauw zijn en de Republikeinen rood. Verder gebruiken we fct_rev om de volgorde van de factoren om te keren. fct_rev zal zo dadelijk worden besproken.

```{r}
library(ggthemes)
survey %>%
	ggplot(aes(party, fill = party_group)) +
	geom_bar() +
	facet_grid(fct_rev(party_group)~., 
	           scales = "free", space = "free") +
	coord_flip() +
	scale_fill_tableau()
```

Als alternatief kunnen we fct_lump gebruiken om een "Andere" categorie te maken. Kijk bijvoorbeeld eens naar de religies.

```{r}
survey %>%
	count(religion)
```

Laten we zeggen dat we alleen de 5 meest voorkomende godsdiensten willen behouden. Dat kunnen we als volgt doen.

```{r}
survey %>%
	mutate(religion = fct_lump(religion, n = 5)) %>%
	count(religion)
```

Het label "Other" kan naar believen worden gewijzigd.

```{r}
survey %>%
	mutate(religion = fct_lump(religion, n = 5, other_level = "Other religions")) %>%
	count(religion)
```

In plaats van het aantal te behouden niveaus op te geven, kan je ook een minimale relatieve frequentie opgeven met het `prop` argument.

```{r}
survey %>%
	mutate(religion = fct_lump(religion, prop = 0.02)) %>%
	count(religion)
```

Meer nog dan het cleanen van data, zijn alle transformaties die we gezien hebben zeer iteratief van aard, en kunnen ze soms alleen gebruikt worden voor specifieke analyses. We willen bijvoorbeeld misschien de weinig voorkomende godsdiensten op één hoop gooien om een staafdiagram te maken zonder al te veel balken -- maar we willen waarschijnlijk niet de weinig voorkomende godsdiensten helemaal verwijderen. Transformaties zullen dus vaak gebeuren in de opbouw naar een grafiek of tabel, en niet permanent in de gegevens worden opgeslagen. Dit geldt vooral voor de laatste functies die we hier zullen bespreken: herordeningsfuncties. 


## Data transformaties tijdens data visualizatie

Bij het visualiseren van categorische variabelen, willen we vaak de volgorde van de niveaus veranderen op basis van frequentie of op basis van een andere variabele. We zagen al `fct_relevel` om handmatig niveaus te herschikken, maar het is niet erg geschikt om automatisch niveaus te herschikken. Hiervoor zullen we twee nieuwe functies gebruiken: fct_infreq en fct_reorder. 

We beginnen met de volgende grafiek.

```{r}
survey %>%
	ggplot(aes(religion)) + 
	geom_bar() +
	coord_flip()
```

Een factor kan worden geordend op basis van de (in)frequentie van elk niveau met behulp van de functie fct_infreq. Het gebruik ervan is eenvoudig. We kunnen kiezen om de functie rechtstreeks in ggplot toe te voegen, of om de religie variabele bij te werken met mutate alvorens te plotten.

```{r}
survey %>%
	ggplot(aes(fct_infreq(religion))) + 
	geom_bar() +
	coord_flip()
```

`fct_infreq` zal voor elk van de labels - religies in dit geval - tellen hoe vaak het voorkomt, en de niveaus overeenkomstig te herschikken. Stel nu dat we geen absolute frequenties willen zoals in de laatste plot, maar relatieve. In dat geval moeten we ze zelf berekenen en `geom_col` gebruiken.

```{r}
survey %>%
	group_by(religion) %>%
	summarise(freq = n()) %>%
	mutate(rel_freq = freq/sum(freq)) %>%
	ggplot(aes(x = religion, y = rel_freq)) +
	geom_col() +
	coord_flip()
```

We kunnen nu de relatieve frequenties uit de grafiek aflezen. Laten we nu de staven nog eens ordenen. 

```{r}
survey %>%
	group_by(religion) %>%
	summarise(freq = n()) %>%
	mutate(rel_freq = freq/sum(freq)) %>%
	ggplot(aes(x = fct_infreq(religion), y = rel_freq)) +
	geom_col() +
	coord_flip()
```

Oeps, dat leek niet te werken. Waarom niet? Laten we eens kijken naar de gegevens die we aan ggplot gaven.

```{r}
survey %>%
	group_by(religion) %>%
	summarise(freq = n()) %>%
	mutate(rel_freq = freq/sum(freq))
```

De gegevens -- een frequentietabel -- bevat een rij voor elke religie. Als we `fct_infreq` gebruiken op deze tabel, dan komen alle religies één keer voor, dus ze komen allemaal even vaak voor. `fct_infreq` probeert impliciet frequenties te berekenen, maar dat hebben we al gedaan. Het resultaat is dat de ordening mislukt. Het is vergelijkbaar met het gebruik van `geom_bar` op basis van een frequentietabel -- we proberen de frequenties twee keer te berekenen, wat resulteert in onbedoelde plots. 

Dus, wat kunnen we in plaats daarvan doen? Wel, we willen de religies ordenen op basis van frequentie. Dat zou niet moeilijk mogen zijn, want de frequentie is er al. De functie `fct_reorder` kan ons daarbij helpen. In tegenstelling tot `fct_infreq` zal deze een tweede variabele gebruiken die we opgeven om de factor te ordenen. Dus: 

```{r}
survey %>%
	group_by(religion) %>%
	summarise(freq = n()) %>%
	mutate(rel_freq = freq/sum(freq)) %>%
	ggplot(aes(x = fct_reorder(religion, rel_freq), y = rel_freq)) +
	geom_col() +
	coord_flip()
```

Merk op dat we tweemaal verwijzen naar rel_freq: eenmaal om te ordenen, en eenmaal om te gebruiken als y-as. Merk ook op dat de volgorde van de staven is omgekeerd: `fct_infreq` zal altijd ordenen van meest naar minst frequent, terwijl onze huidige configuratie met `fct_reorder` ordent van minst naar meest frequent. We kunnen dit eenvoudig veranderen met desc(), zoals we eerder hebben gedaan. 

```{r}
survey %>%
	group_by(religion) %>%
	summarise(freq = n()) %>%
	mutate(rel_freq = freq/sum(freq)) %>%
	ggplot(aes(x = fct_reorder(religion, desc(rel_freq)), y = rel_freq)) +
	geom_col() +
	coord_flip()
```

Als alternatief kunnen we de `fct_rev` functie gebruiken: deze functie zal de volgorde van een factor omkeren. Zo kunnen we bijvoorbeeld de volgorde van `fct_infreq` omkeren. 

```{r}
survey %>%
	ggplot(aes(fct_rev(fct_infreq(religion)))) + 
	geom_bar() +
	coord_flip()
```

Dus, samenvattend:

* `fct_infreq(f)`: herorden de niveaus van factor f van meest naar minst frequent
* `fct_reorder(f, x)`: herschik de niveaus van factor f volgens variabele x
* `fct_rev(f)`: draai de volgorde van de niveaus van factor f om

In het algemeen moet je `fct_infreq` alleen gebruiken op de originele gegevens, en als je een frequentietabel als invoer hebt voor ggplot, moet je `fct_reorder` gebruiken. 

Er is echter nog één ding dat we moeten behandelen. `fct_reorder(f,x)` kan herordenen op elke variabele x, niet alleen op frequentie. Kijk bijvoorbeeld eens naar de volgende plot. 

```{r}
survey %>%
	ggplot(aes(marital, age)) +
	geom_boxplot()
```

Deze plot toont de verdeling van de leeftijd voor verschillende burgerlijke standen. Laten we zeggen dat we deze boxplots willen sorteren op leeftijd. We gebruiken `fct_reorder` net zoals we eerder deden.

```{r}
survey %>%
	ggplot(aes(fct_reorder(marital, age), age)) +
	geom_boxplot()
```


Ook hier zijn de resultaten niet zoals we zouden verwachten. Wat is er anders dan bij het vorige gebruik met frequenties?

There are actually two differences.

1. 	Vroeger hadden we één enkele frequentie voor elke godsdienst, waardoor het gemakkelijk was ze te sorteren.

Nu hebben we voor elke burgerlijke staat veel personen met verschillende leeftijden. We moeten ze samenvatten in een enkele waarde, zoals het gemiddelde of de mediaan. Dit kan gedaan worden door het .fun argument toe te voegen aan factor reorder.

fct_reorder(marital, age, .fun = median)

2. 	Sommige personen hebben geen leeftijd, maar een ontbrekende waarde. 

Het berekenen van een functie met ontbrekende waarden leidt tot een ontbrekende waarde. We moeten ervoor zorgen dat ontbrekende waarden worden genegeerd. Elk argument dat we toevoegen aan fct_reorder na het .fun argument zal worden beschouwd als een argument voor deze functie. Dus, het volgende zal de marital waarden correct sorteren

fct_reorder(marital, age, .fun = median, na.rm = T)

Laat ons het proberen: 

```{r}
survey %>%
	ggplot(aes(fct_reorder(marital, age, .fun = median, na.rm = T), age)) +
	geom_boxplot()
```

Dit ziet er beter uit. Dus, laten we het nog eens samenvatten. 

* `fct_infreq(f)`: herorden de niveaus van factor f van meest naar minst frequent
* `fct_reorder`: herschik de niveaus van factor f volgens variabele x
	* `fct_reorder(f,x)` als we zeker weten dat er een enkele x-waarde is voor elk f-niveau
	* `fct_reorder(f,x, .fun = summarize_function)` als er meer dan één x-waarde kan zijn voor een bepaald f-niveau. De `summarize_function` zal worden gebruikt om meerdere waarden te combineren. Dit kan zijn gemiddelde, mediaan, som, lengte, ... elke functie die een enkele waarde teruggeeft.
	* Als we extra argumenten aan de summarize functie moeten meegeven, zoals na.rm = T, kunnen we dit als volgt doen: `fct_reorder(f,x, .fun = summarize_function, na.rm = T)`.
* `fct_rev(f)`: draai de volgorde van de niveaus van factor f om

## Achtergrondmateriaal 

* Meer informatie over forcats kan gelezen worden in Hoofdstuk 15 van [R for Data Science](https://r4ds.had.co.nz/factors.html)
